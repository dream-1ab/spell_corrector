{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "745cff4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, Encoding, pre_tokenizers, decoders, Regex, CharBPETokenizer\n",
    "\n",
    "tokenizer: Tokenizer = CharBPETokenizer(unk_token=\"<UNK>\")\n",
    "\n",
    "tokenizer.add_special_tokens([\"<PAD>\", \"<UNK>\", \"<SOS>\", \"EOS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "001e758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../../config/symbols.json\", \"r\") as f:\n",
    "    vocab_list: list[str] = json.loads(f.read())\n",
    "    \n",
    "    tokenizer.add_tokens(vocab_list)\n",
    "tokenizer.save(\"../../config/input_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad73eb9",
   "metadata": {},
   "source": [
    "### Load generated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64fe8cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer: Tokenizer = Tokenizer.from_file(\"../../config/input_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64e5ece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: [35, 4, 11, 17, 34, 25, 29, 16, 34, 14, 38, 50, 20, 4, 26, 12, 4, 20, 50, 36, 5, 27, 32, 4, 24, 34, 23, 34, 14, 39, 50, 36, 4, 35, 12, 34, 23, 24, 4, 17, 8, 29, 13, 4, 24, 25, 4, 35, 32, 4, 8, 20, 4, 26, 24, 34, 20, 34, 23, 34, 14, 12, 34, 26, 25, 34, 21, 34, 26, 8, 4, 23, 50, 36, 4, 35, 12, 34, 23, 24, 4, 17, 8, 29, 13, 4, 24, 25, 4, 35, 32, 4, 8, 20, 4, 26, 12, 5, 21, 50, 20, 34, 24, 34, 16, 34, 14, 89]\n",
      "encoded: ['ي', 'ا', 'خ', 'ش', 'ى', 'م', 'ۇ', 'س', 'ى', 'ز', '،', ' ', 'ق', 'ا', 'ن', 'د', 'ا', 'ق', ' ', 'ئ', 'ە', 'ھ', 'ۋ', 'ا', 'ل', 'ى', 'ڭ', 'ى', 'ز', '؟', ' ', 'ئ', 'ا', 'ي', 'د', 'ى', 'ڭ', 'ل', 'ا', 'ش', 'ت', 'ۇ', 'ر', 'ا', 'ل', 'م', 'ا', 'ي', 'ۋ', 'ا', 'ت', 'ق', 'ا', 'ن', 'ل', 'ى', 'ق', 'ى', 'ڭ', 'ى', 'ز', 'د', 'ى', 'ن', 'م', 'ى', 'ك', 'ى', 'ن', 'ت', 'ا', 'ڭ', ' ', 'ئ', 'ا', 'ي', 'د', 'ى', 'ڭ', 'ل', 'ا', 'ش', 'ت', 'ۇ', 'ر', 'ا', 'ل', 'م', 'ا', 'ي', 'ۋ', 'ا', 'ت', 'ق', 'ا', 'ن', 'د', 'ە', 'ك', ' ', 'ق', 'ى', 'ل', 'ى', 'س', 'ى', 'ز', '.']\n",
      "len of tokens: 108\n",
      "Source : ياخشىمۇسىز، قانداق ئەھۋالىڭىز؟ ئايدىڭلاشتۇرالمايۋاتقانلىقىڭىزدىنمىكىنتاڭ ئايدىڭلاشتۇرالمايۋاتقاندەك قىلىسىز.\n",
      "Decoded: ياخشىمۇسىز، قانداق ئەھۋالىڭىز؟ ئايدىڭلاشتۇرالمايۋاتقانلىقىڭىزدىنمىكىنتاڭ ئايدىڭلاشتۇرالمايۋاتقاندەك قىلىسىز.\n"
     ]
    }
   ],
   "source": [
    "text = \"ياخشىمۇسىز، قانداق ئەھۋالىڭىز؟ ئايدىڭلاشتۇرالمايۋاتقانلىقىڭىزدىنمىكىنتاڭ ئايدىڭلاشتۇرالمايۋاتقاندەك قىلىسىز.\"\n",
    "encoded: Encoding = tokenizer.encode(text)\n",
    "print(f\"ids: {encoded.ids}\")\n",
    "print(f\"encoded: {encoded.tokens}\")\n",
    "decoded: str = tokenizer.decode(encoded.ids)\n",
    "print(f\"len of tokens: {len(encoded.ids)}\")\n",
    "print(f\"Source : {text}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spell_correction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
