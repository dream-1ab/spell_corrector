{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c66c05bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tokenizers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01msys\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tokenizers'"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path(Path.cwd()).parent))\n",
    "\n",
    "from helper.sentence_destructor import destruct_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52f2cc3",
   "metadata": {},
   "source": [
    "### Test dataset destruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558c577c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مەنامەكبپەپت<r قايتىنپ كەلدىم، ئۇمۇ مەكاتەپتفىن tقايتىلز كەلدى، بىز ھممىمىز مەكتjتىن قاي)تىپ كەلدۇق!\n",
      "مەن مڭكتە›پتى قايتى كەلىم، ئۇمۇ مەكتەپتىن ق1يتىلqكەلدى، بىز ھەممشمىز مەكتەپتىن قايتىپ كەلدcدۇ<قس\n",
      "مەن مەكتەپتى قايتىپ كەدىمف، ئ9ۇمۇ8مەكتەپتىن قيتوىل6كtلدى، بىز ھ=ممىمىز مەlتپتىن قايتىپ كە دۇd!\n",
      "مەن مەتەپتىن.قايتىپ$ەلدىم، ئۇمۇ مە8كتەتىن قايتىل كەلدى0 بىزھەممىمىز مەتەپتىنc قايتىپ كەلدۇe!\n",
      "!ەن مكkەپتىن قايتىp كەwلدى5g،ئۇۇ مەكتەپتىن قايتىل-ەلدى، بىز ھەمم$مىز مكتەپتىن 7ايتىپ كەلدۇق!\n",
      "مەن مەكتەپتىن قايتىپ كەلدىم،t ئۇمۇ مەكەپتىن قايتىq كە8لفى،tgىز ھەممnىامىزtمەكتلپتىن dقايتىپ كەلدۇق!\n",
      "من مەكتە-تىن قايتىپ كەلدىم، ئۇمۇ مەكتەپت4ن ق۔يت\"ىل كەلدى5،بىزھەمم?مىز› ەكتەپتىن قايتپ كەلدۇ۔ق!\n",
      "م7ن مەكتەپتىن پقايتىپ كەلدىم، ۇمشۇ [ەكتەپتىن قا^يتىj %كەلدى، بىز ھممىمىز مە*تەپتى[قغيتىپ كەلدۇق!\n",
      "مە~ مەكتەپmىqن قاي£تىپ كەلدىم،]ۇمۇ مەتەپىن قايتىل كەلدى، بىز ھەممىمىoز مەكتەپۆىنقايتىپ كەلدlۇق_!\n",
      "مەن مە«كتەپت؛تىن قايتىپكەلدىم، ئۇمۇ مەكتەتىن قايتىل كەلدى، بچsز ھەممىمgز مەكە[تىن قا«تپ كە#د=ق!\n",
      "مەن مەكتەپتىن قايتىپ كەلدىم، ئۇمۇ مەكتەپتىن قايتىل كەلدى، بىز ھەممىمىز مەكتەپتىن قايتىپ كەلدۇق!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer: Tokenizer = Tokenizer.from_file(\"../../config/input_tokenizer.json\")\n",
    "\n",
    "text = \"مەن مەكتەپتىن قايتىپ كەلدىم، ئۇمۇ مەكتەپتىن قايتىل كەلدى، بىز ھەممىمىز مەكتەپتىن قايتىپ كەلدۇق!\"\n",
    "\n",
    "for i in range(10):\n",
    "    destructed = destruct_sentence(tokenizer, text, 0.15)\n",
    "\n",
    "    print(destructed)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107144b9",
   "metadata": {},
   "source": [
    "### Shirnk sentence length by split sentences as sub-sentences by splitting their boundaries using dot (.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "995f10c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence count before split: 12503\n",
      "sentence count after split: 22262\n",
      "max sentence length after split: 1019\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data_dir = Path(Path.cwd()).parent.parent / \".data\"\n",
    "dataset_file = data_dir / \"text\" / \"2000.json\"\n",
    "with open(dataset_file, \"r\") as file:\n",
    "    sentences: list[str] = json.loads(file.read())\n",
    "print(f\"sentence count before split: {len(sentences)}\")\n",
    "expanded_sentences: list[str] = []\n",
    "\n",
    "for s in sentences:\n",
    "    ss = [ss for ss in s.strip().split(\".\") if not (ss.strip() == \"\")]\n",
    "    ss = [s for s in ss if len(s) > 15]\n",
    "    expanded_sentences.extend(ss)\n",
    "print(f\"sentence count after split: {len(expanded_sentences)}\")\n",
    "\n",
    "max_sentence_length = 0\n",
    "for s in expanded_sentences:\n",
    "    max_sentence_length = max(max_sentence_length, len(s))\n",
    "\n",
    "print(f\"max sentence length after split: {max_sentence_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd7aef",
   "metadata": {},
   "source": [
    "### Write the shirnked data to a temporary working file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e74ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] of write shirnked sentence to file.\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir / \"dataset.json\", \"w+\") as file:\n",
    "    file.write(json.dumps(expanded_sentences, ensure_ascii=False))\n",
    "print(\"[OK] of write shirnked sentence to file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c183f0",
   "metadata": {},
   "source": [
    "### ChatGPT says we can minimize memory re-allocation overhead by pre-allocate fixed sized buffer on the GPU and copy the data into gpu before model call in training loop. lets do some benchmark to validate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2575373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to taken by copy data to buffer 2000 times: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "my_device = \"cuda:0\"\n",
    "\n",
    "dummy_data1 = torch.randint(-32768, 32767, (1024, 2048, 128), dtype=torch.int32, device=my_device)\n",
    "dummy_data2 = torch.randint(-32768, 32767,(1024, 2048, 128), dtype=torch.int32, device=my_device)\n",
    "\n",
    "my_buffer = torch.zeros((1024, 2048, 128), dtype=torch.int32, device=my_device)\n",
    "\n",
    "begin = time.time()\n",
    "for i in range(100):\n",
    "    my_buffer[:, :, :] = dummy_data1[:, :, :]\n",
    "    my_buffer[:, :, :] = dummy_data2[:, :, :]\n",
    "elapsed_ms = int((time.time() - begin) * 100)\n",
    "\n",
    "del dummy_data1, dummy_data2, my_buffer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"time to taken by copy data to buffer 2000 times: {elapsed_ms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5f837",
   "metadata": {},
   "source": [
    "### Let's see how much vram taken by all the sentences if we move them into GPU entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c004bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "training_data = torch.zeros(len(expanded_sentences), max_sentence_length, dtype=torch.int32, device=\"cuda:0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
